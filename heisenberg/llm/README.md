# Module LLM - Heisenberg

Module d'intÃ©gration de modÃ¨les de langage locaux pour Heisenberg via llama.cpp.

## ðŸ“ Structure

```
heisenberg/llm/
â”œâ”€â”€ __init__.py          # Module exports
â”œâ”€â”€ stream.py            # Client LLM (LlamaCppLLM)
â””â”€â”€ prompts.py           # SystÃ¨me de prompts (PromptBuilder)
```

## ðŸŽ¯ ResponsabilitÃ©s

### `stream.py` - Client LLM

**Classe principale:** `LlamaCppLLM`

GÃ¨re la communication avec le serveur llama.cpp:
- RequÃªtes HTTP asynchrones (aiohttp)
- Parsing du stream SSE (Server-Sent Events)
- Callbacks pour tokens et complÃ©tion
- Gestion timeout et annulation

**Usage:**

```python
from heisenberg.llm.stream import LlamaCppLLM
from heisenberg.core.config import Config

config = Config.load()
llm = LlamaCppLLM(config.llm)

# Streaming
async for token in llm.generate("Bonjour"):
    print(token, end='', flush=True)

# Non-streaming (convenience)
response = await llm.generate_simple("Quelle heure est-il ?")
print(response)

# Avec historique
history = [("Qui es-tu ?", "Je suis Heisenberg.")]
response = await llm.generate_simple(
    "Rappelle-moi ton nom",
    conversation_history=history
)
```

### `prompts.py` - Construction de Prompts

**Classe principale:** `PromptBuilder`

Construit des prompts formatÃ©s avec historique conversationnel:
- 3 formats supportÃ©s (plain, chatml, llama2)
- Gestion automatique du contexte
- PersonnalitÃ©s prÃ©dÃ©finies

**Usage:**

```python
from heisenberg.llm.prompts import PromptBuilder, SYSTEM_PROMPTS

# CrÃ©er un builder
builder = PromptBuilder(
    system_prompt=SYSTEM_PROMPTS["concise"],
    format_style="plain"
)

# Construire un prompt avec historique
history = [
    ("Bonjour", "Salut, comment puis-je t'aider ?"),
]
prompt = builder.build(history, "Quelle est la mÃ©tÃ©o ?")

# RÃ©sultat:
# System: Tu es un assistant vocal. RÃ©ponds en 1-2 phrases maximum.
#
# User: Bonjour
# Assistant: Salut, comment puis-je t'aider ?
#
# User: Quelle est la mÃ©tÃ©o ?
# Assistant:
```

## ðŸ”§ Configuration

Voir `heisenberg/core/config.py`:

```python
@dataclass
class LLMConfig:
    endpoint: str = "http://localhost:8080/completion"
    model_name: str = "LFM2-350M"
    temperature: float = 0.7
    max_tokens: int = 512
    top_p: float = 0.9
    top_k: int = 40
    repeat_penalty: float = 1.1
    timeout_seconds: int = 30
    system_prompt: str = "..."
    max_history_turns: int = 5
```

## ðŸ§ª Tests

```bash
# Suite complÃ¨te
uv run heisenberg/tests/test_llm.py

# Test individuel
uv run python -c "
import asyncio
from heisenberg.llm.stream import LlamaCppLLM
from heisenberg.core.config import Config

async def test():
    llm = LlamaCppLLM(Config.load().llm)
    response = await llm.generate_simple('Bonjour')
    print(response)

asyncio.run(test())
"
```

## ðŸ“š Documentation

- **Guide utilisateur:** [`docs/LLM_GUIDE.md`](../../docs/LLM_GUIDE.md)
- **Architecture:** [`docs/LLM_ARCHITECTURE.md`](../../docs/LLM_ARCHITECTURE.md)
- **IntÃ©gration complÃ¨te:** [`docs/INTEGRATION_COMPLETE.md`](../../docs/INTEGRATION_COMPLETE.md)

## ðŸ”— IntÃ©gration dans Heisenberg

Le module s'intÃ¨gre dans le flux principal via `main.py`:

```python
# 1. Initialisation
llm_engine = LlamaCppLLM(config.llm, prompt_builder)

# 2. Callback aprÃ¨s transcription
async def on_transcription_final(text: str):
    # RÃ©cupÃ©rer l'historique
    history = fsm.session_manager.get_conversation_history(max_turns=5)
    
    # GÃ©nÃ©rer rÃ©ponse
    llm_response = ""
    async for token in llm_engine.generate(text, history):
        llm_response += token
        # TODO: Stream vers TTS
    
    # Sauvegarder dans l'historique
    fsm.session_manager.add_conversation_turn(text, llm_response)
```

## ðŸŽ¨ PersonnalitÃ©s PrÃ©dÃ©finies

```python
SYSTEM_PROMPTS = {
    "default": "Assistant Ã©quilibrÃ© et serviable",
    "concise": "RÃ©ponses ultra-courtes (1-2 phrases)",
    "friendly": "Ton chaleureux et dÃ©contractÃ©",
    "professional": "Formel et structurÃ©",
    "technical": "DÃ©taillÃ© avec termes techniques",
}
```

## ðŸš€ Optimisations

### Latence
- Streaming activÃ© par dÃ©faut
- Timeout configurable
- Prompt caching (cÃ´tÃ© llama.cpp)

### MÃ©moire
- FenÃªtre glissante d'historique (`max_history_turns`)
- Pas de stockage des tokens intermÃ©diaires

### QualitÃ©
- `temperature` ajustable (0.0-1.0)
- `repeat_penalty` pour Ã©viter rÃ©pÃ©titions
- Multiple formats de prompts selon le modÃ¨le

## ðŸ› DÃ©pannage

### `aiohttp.ClientConnectorError`
â†’ llama-server n'est pas lancÃ©
```bash
./start_llama_server.sh models/lfm2-350m.gguf
```

### `asyncio.TimeoutError`
â†’ Augmenter `timeout_seconds` dans LLMConfig

### RÃ©ponses vides
â†’ VÃ©rifier le `format_style` (essayer "plain", "chatml", "llama2")

### RÃ©pÃ©titions
â†’ Augmenter `repeat_penalty` (1.1 â†’ 1.3)

## ðŸ“¦ DÃ©pendances

- `aiohttp>=3.9.0`: Client HTTP asynchrone
- **Externe:** `llama.cpp` (llama-server)

## ðŸ”® Ã‰volutions Futures

- [ ] Support multi-modÃ¨les (switch dynamique)
- [ ] RAG (Retrieval Augmented Generation)
- [ ] Function calling / Tools
- [ ] Compression automatique d'historique
- [ ] MÃ©triques (latence, tokens/sec)
- [ ] Cache de prompts intelligent

## ðŸ“„ License

Voir [LICENSE](../../LICENSE) Ã  la racine du projet.


