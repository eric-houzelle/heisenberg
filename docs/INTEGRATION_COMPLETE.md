# âœ… IntÃ©gration LLM ComplÃ¨te

## ğŸ“‹ RÃ©sumÃ©

L'intÃ©gration du module LLM dans Heisenberg est **terminÃ©e** ! Vous pouvez maintenant utiliser un modÃ¨le de langage local (LFM2-350M) via llama.cpp pour rÃ©pondre aux requÃªtes vocales.

## ğŸ¯ FonctionnalitÃ©s ImplÃ©mentÃ©es

âœ… **Client LLM asynchrone**
- Communication HTTP avec llama.cpp
- Streaming de tokens en temps rÃ©el (SSE)
- Gestion des timeouts et erreurs
- Support de l'annulation

âœ… **SystÃ¨me de prompts flexible**
- Construction automatique avec historique
- 3 formats supportÃ©s (Plain, ChatML, Llama2)
- 5 personnalitÃ©s prÃ©dÃ©finies
- Personnalisation facile

âœ… **Historique conversationnel**
- Stockage des tours de conversation
- FenÃªtre glissante configurable
- IntÃ©grÃ© dans SessionManager
- Persistance pendant la session

âœ… **IntÃ©gration FSM**
- Ã‰vÃ©nements LLM_TOKEN et LLM_COMPLETE
- Transition LISTENING â†’ THINKING â†’ IDLE
- Gestion d'erreurs robuste
- Logging dÃ©taillÃ©

âœ… **Tests et documentation**
- Suite de tests complÃ¨te
- Documentation utilisateur (LLM_GUIDE.md)
- Scripts de dÃ©marrage
- Exemples de configuration

## ğŸ“ Structure des Fichiers

```
heisenberg/
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ stream.py          # â­ Client LlamaCppLLM
â”‚   â””â”€â”€ prompts.py         # â­ PromptBuilder & personnalitÃ©s
â”œâ”€â”€ orchestrator/
â”‚   â”œâ”€â”€ session.py         # â­ Historique conversationnel
â”‚   â””â”€â”€ ...
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ config.py          # â­ LLMConfig ajoutÃ©e
â”‚   â””â”€â”€ ...
â”œâ”€â”€ main.py                # â­ IntÃ©gration complÃ¨te
â””â”€â”€ tests/
    â””â”€â”€ test_llm.py        # â­ Suite de tests

docs/
â”œâ”€â”€ LLM_GUIDE.md           # â­ Guide utilisateur complet
â””â”€â”€ LLM_INTEGRATION_SUMMARY.md

Scripts:
â”œâ”€â”€ start_llama_server.sh  # â­ DÃ©marrage llama.cpp
â””â”€â”€ setup_guide.sh         # â­ Guide de configuration

Config:
â”œâ”€â”€ config.example.toml    # â­ Template configuration
â””â”€â”€ pyproject.toml         # â­ aiohttp ajoutÃ©
```

## ğŸš€ DÃ©marrage Rapide

### 1. Installer les dÃ©pendances

```bash
# Installer llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make llama-server
sudo cp llama-server /usr/local/bin/

# Installer les dÃ©pendances Python
cd /path/to/heisenberg/app
uv sync
```

### 2. TÃ©lÃ©charger le modÃ¨le

```bash
mkdir -p models
# TÃ©lÃ©chargez votre LFM2-350M.gguf ici
```

### 3. DÃ©marrer

```bash
# Terminal 1: Serveur LLM
./start_llama_server.sh models/lfm2-350m-q8_0.gguf

# Terminal 2: Test LLM seul
uv run heisenberg/tests/test_llm.py

# Terminal 3: Heisenberg complet
uv run heisenberg/main.py
```

## ğŸ”§ Configuration

ParamÃ¨tres principaux dans `heisenberg/core/config.py`:

```python
@dataclass
class LLMConfig:
    endpoint: str = "http://localhost:8080/completion"
    temperature: float = 0.7      # CrÃ©ativitÃ©
    max_tokens: int = 512         # Longueur max
    max_history_turns: int = 5    # Tours mÃ©morisÃ©s
    system_prompt: str = "..."    # PersonnalitÃ©
```

## ğŸ“Š MÃ©triques de Performance

**Latence attendue avec LFM2-350M:**
- First token (TTFT): ~100-300ms (CPU) / ~50-100ms (GPU)
- GÃ©nÃ©ration: ~20-50 tokens/sec (CPU) / ~100+ tokens/sec (GPU)

**Optimisations:**
- âœ… Streaming activÃ© (pas d'attente de rÃ©ponse complÃ¨te)
- âœ… Prompt caching possible (llama.cpp)
- ğŸ”œ Streaming vers TTS (Ã  implÃ©menter)

## ğŸ”„ Flux d'ExÃ©cution

```
User dit: "Hey Jarvis"
    â†“
[WAKEWORD_DETECTED]
    â†“
State: IDLE â†’ LISTENING
    â†“
User dit: "Quelle est la capitale de la France ?"
    â†“
Whisper transcrit
    â†“
[TRANSCRIPTION_FINAL] "Quelle est la capitale de la France ?"
    â†“
State: LISTENING â†’ THINKING
    â†“
LlamaCppLLM.generate()
  - RÃ©cupÃ¨re historique (5 derniers tours)
  - Construit prompt avec PromptBuilder
  - Envoie Ã  llama.cpp
  - Stream tokens: "La" â†’ "capitale" â†’ "de" â†’ "la" â†’ ...
    â†“
[LLM_TOKEN] Premier token reÃ§u
    â†“
[LLM_COMPLETE] "La capitale de la France est Paris."
    â†“
SessionManager.add_turn(query, response)
    â†“
State: THINKING â†’ IDLE (via TTS dans le futur)
    â†“
PrÃªt pour prochaine requÃªte
```

## ğŸ§ª Tests Disponibles

```bash
# Test 1: Query simple
uv run heisenberg/tests/test_llm.py
# VÃ©rifie: Connexion, gÃ©nÃ©ration basique

# Test 2: Dans le code
from heisenberg.llm.stream import LlamaCppLLM
llm = LlamaCppLLM(config.llm)
response = await llm.generate_simple("Bonjour")

# Test 3: Avec streaming
async for token in llm.generate("Raconte une blague"):
    print(token, end='', flush=True)

# Test 4: Avec historique
history = [("Qui es-tu ?", "Je suis Heisenberg.")]
response = await llm.generate_simple(
    "Rappelle-moi ton nom", 
    conversation_history=history
)
```

## ğŸ¨ PersonnalitÃ©s Disponibles

```python
from heisenberg.llm.prompts import SYSTEM_PROMPTS, PromptBuilder

# Concis (1-2 phrases max)
builder = PromptBuilder(SYSTEM_PROMPTS["concise"], "plain")

# Amical et dÃ©contractÃ©
builder = PromptBuilder(SYSTEM_PROMPTS["friendly"], "plain")

# Professionnel et formel
builder = PromptBuilder(SYSTEM_PROMPTS["professional"], "plain")

# Technique et dÃ©taillÃ©
builder = PromptBuilder(SYSTEM_PROMPTS["technical"], "plain")
```

## ğŸ“ Prochaines Ã‰tapes (TODO)

### PrioritÃ© Haute
ğŸ”œ **Module TTS** - SynthÃ¨se vocale des rÃ©ponses
ğŸ”œ **Streaming LLM â†’ TTS** - Parler pendant la gÃ©nÃ©ration
ğŸ”œ **Barge-in** - Interrompre l'assistant

### PrioritÃ© Moyenne
- Configuration externe (TOML/YAML)
- Skills/Plugins systÃ¨me (mÃ©tÃ©o, timer, etc.)
- Compression d'historique automatique
- MÃ©triques de performance (latence, tokens/sec)

### PrioritÃ© Basse
- Multi-utilisateurs avec profils
- RAG (Retrieval Augmented Generation)
- Fine-tuning du modÃ¨le
- Interface web de configuration

## ğŸ› Troubleshooting

### "Connection refused" lors du test LLM
â†’ VÃ©rifiez que llama-server tourne sur le port 8080
```bash
lsof -i :8080
./start_llama_server.sh
```

### Timeout lors de la gÃ©nÃ©ration
â†’ Augmentez le timeout dans la config
```python
timeout_seconds: int = 60
```

### RÃ©ponses incohÃ©rentes
â†’ VÃ©rifiez le format de prompt (`format_style`)
â†’ Testez avec `temperature: 0.5` (plus dÃ©terministe)
â†’ RÃ©duisez `max_history_turns` si contexte trop grand

### Latence Ã©levÃ©e
â†’ Utilisez un modÃ¨le quantifiÃ© (q8_0, q4_k_m)
â†’ Activez GPU: `--n-gpu-layers 99`
â†’ RÃ©duisez `max_tokens`

## ğŸ“š Documentation

- **[LLM_GUIDE.md](LLM_GUIDE.md)** - Guide complet avec exemples
- **[README.md](../README.md)** - Documentation principale Heisenberg
- **Code docstrings** - Documentation inline dans le code

## âœ¨ Conclusion

Le module LLM est **opÃ©rationnel et prÃªt Ã  l'emploi** ! 

FonctionnalitÃ©s core:
- âœ… GÃ©nÃ©ration de texte en streaming
- âœ… Historique conversationnel
- âœ… Multiple personnalitÃ©s
- âœ… IntÃ©gration FSM complÃ¨te
- âœ… Tests et documentation

Il ne reste plus qu'Ã  :
1. TÃ©lÃ©charger votre modÃ¨le LFM2-350M
2. DÃ©marrer llama-server
3. Profiter de votre assistant vocal intelligent ! ğŸ‰

---

**Questions ?** Consultez la documentation ou les tests pour des exemples concrets.


