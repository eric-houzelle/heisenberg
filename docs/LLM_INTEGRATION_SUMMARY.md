# IntÃ©gration LLM - RÃ©sumÃ© Technique

## ğŸ¯ Objectif Accompli

IntÃ©gration complÃ¨te d'un modÃ¨le de langage local (LFM2-350M) dans Heisenberg via llama.cpp, avec support du streaming et de l'historique conversationnel.

## ğŸ“¦ Fichiers CrÃ©Ã©s/ModifiÃ©s

### Nouveaux Fichiers
1. **`heisenberg/llm/stream.py`** - Client LLM avec streaming
   - Classe `LlamaCppLLM` pour communication avec llama.cpp
   - Support streaming token-par-token via SSE
   - Callbacks pour `on_token` et `on_complete`

2. **`heisenberg/llm/prompts.py`** - SystÃ¨me de prompts
   - Classe `PromptBuilder` pour construction de prompts
   - Support multi-formats (ChatML, Llama2, Plain)
   - 5 personnalitÃ©s prÃ©dÃ©finies (default, concise, friendly, etc.)

3. **`heisenberg/tests/test_llm.py`** - Suite de tests
   - Test query simple
   - Test streaming
   - Test avec historique conversationnel

4. **`docs/LLM_GUIDE.md`** - Documentation complÃ¨te
   - Installation et configuration
   - Guide d'utilisation
   - Troubleshooting

5. **`start_llama_server.sh`** - Script de dÃ©marrage
   - Lancement automatique du serveur llama.cpp
   - Configuration optimisÃ©e pour LFM2-350M

6. **`config.example.toml`** - Config exemple
   - Template de configuration externe

### Fichiers ModifiÃ©s
1. **`heisenberg/core/config.py`**
   - Ajout de `LLMConfig` avec tous les paramÃ¨tres

2. **`heisenberg/orchestrator/session.py`**
   - Extension pour historique conversationnel
   - MÃ©thodes `add_conversation_turn()` et `get_conversation_history()`

3. **`heisenberg/main.py`**
   - IntÃ©gration du LLM dans le flux principal
   - Gestion des Ã©vÃ©nements `LLM_TOKEN` et `LLM_COMPLETE`
   - Streaming des rÃ©ponses avec historique

4. **`pyproject.toml`**
   - Ajout de la dÃ©pendance `aiohttp>=3.9.0`

5. **`README.md`**
   - Mise Ã  jour avec mention du module LLM

## ğŸ”„ Flux d'ExÃ©cution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     HEISENBERG WORKFLOW                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. IDLE State
   â””â”€> Wakeword Engine Ã©coute en continu
   
2. WAKEWORD_DETECTED Event
   â””â”€> Transition: IDLE â†’ LISTENING
   â””â”€> STT dÃ©marre l'enregistrement
   
3. LISTENING State
   â””â”€> VAD dÃ©tecte la parole
   â””â”€> Audio stream vers Whisper
   â””â”€> DÃ©tection de silence â†’ stop STT
   
4. TRANSCRIPTION_FINAL Event
   â””â”€> Transition: LISTENING â†’ THINKING
   â””â”€> RÃ©cupÃ©ration historique conversationnel
   â””â”€> Construction du prompt avec PromptBuilder
   â””â”€> Envoi requÃªte HTTP Ã  llama.cpp
   
5. THINKING State (LLM Generation)
   â”œâ”€> Premier token â†’ Event.LLM_TOKEN
   â”œâ”€> Tokens streamÃ©s en continu
   â””â”€> Fin gÃ©nÃ©ration â†’ Event.LLM_COMPLETE
   
6. Post-LLM Processing
   â”œâ”€> Sauvegarde du tour dans session history
   â”œâ”€> [TODO] Envoi vers TTS
   â””â”€> Transition: THINKING â†’ IDLE
   
7. Retour Ã  IDLE
   â””â”€> PrÃªt pour prochaine requÃªte
```

## ğŸ—ï¸ Architecture Technique

### Communication LLM

```python
# 1. Client HTTP asynchrone (aiohttp)
async with session.post(endpoint, json=payload) as response:
    
    # 2. Parsing SSE (Server-Sent Events)
    async for line in response.content:
        data = json.loads(line[6:])  # Remove "data: " prefix
        token = data['content']
        
        # 3. Yield token pour streaming
        yield token
```

### Gestion de l'historique

```python
# SessionManager stocke les tours de conversation
session.add_turn(
    user_query="Quelle est la capitale de la France ?",
    assistant_response="La capitale de la France est Paris."
)

# PromptBuilder construit le prompt avec contexte
history = session.get_history(max_turns=5)
prompt = builder.build(history, current_query)
```

### Formats de prompts supportÃ©s

**Plain Text** (recommandÃ© pour LFM2):
```
System: Tu es un assistant...

User: Bonjour

